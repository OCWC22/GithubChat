
# solve cases

1. Ask explain a function. For instance, "how to use LLMAsRetriever"?
2. need memory so that users can ask follow up questions.



# Evaluation dataset 

1. How to evaluate if the RAG is working well for users who wants to query a github repo?

As we can not expect the the LLM can answer the question if it is not retrieved in the context,
we need to evaluate both the retriever and the generator.